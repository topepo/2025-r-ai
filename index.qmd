---
title: "Analyzing LLM Evaluations"
subtitle: "2025 R+AI Conference"
author: "Max Kuhn and Simon Couch"
institute: "Posit PBC"
---

# Sides + sources on github: <br> `topepo/2025-r-ai`  {background-color="#B25D91FF"}

```{r}
#| label: pkgs
#| results: hide
#| echo: false
# pak::pak(c("https://github.com/tidyverse/vitals"), ask = FALSE)
library(vitals) 
# For analysis:
library(tidymodels)
library(ordinal)
library(brms)
library(RcppEigen)
library(broom)
# For presentation:
library(cli)
library(gt)
library(ggridges)
```

```{r}
#| label: vitials
#| results: hide
#| echo: false

"https://github.com/tidyverse/vitals/raw/refs/heads/main/vignettes/articles/data/analysis/are_claude.rda" |> 
 url() |> 
 load()

"https://github.com/tidyverse/vitals/raw/refs/heads/main/vignettes/articles/data/analysis/are_gemini.rda" |> 
 url() |> 
 load()

"https://github.com/tidyverse/vitals/raw/refs/heads/main/vignettes/articles/data/analysis/are_gpt.rda" |> 
 url() |> 
 load()

are_eval <-
  vitals_bind(
    `Claude 4 Sonnet` = are_claude,
    `GPT 4.1` = are_gpt,
    `Gemini 2.5 Pro` = are_gemini
  ) |>
  rename(LLM = task)

are_eval

leaders <-
  are_eval %>%
  summarize(correct = mean(score == "C"), .by = c(LLM)) %>%
  arrange(correct)

questions <- 
 are_eval %>%
 summarize(correct = mean(score == "C"), .by = c(id)) %>%
 arrange(correct)

are_eval <- are_eval %>%
 mutate(
  LLM = factor(LLM, leaders$LLM),
  id = factor(id, questions$id)
 )
```

## How do we compare LLMs, prompts, agents, etc?

> How do I add a calibration model to an XGBoost classifier using tidymodels?

If I get results for this using different LLMs or prompts, how can I know

- Which setup works best? 

- Do the results improve over time? 

- How stochastic are the results? 

See ["I was wrong about tidymodels and LLMs"](https://www.simonpcouch.com/blog/2025-08-26-predictive/) by Simon for an interesting read. 

## An example endpoint

```{r}
#| label: example-plot
#| echo: false
#| fig-width: 6
#| fig-height: 3
#| fig-align: "center"
#| out-width: "50%"

set.seed(382)
tibble(model = paste("LLM", 1:6)) |> 
 dplyr::mutate(
  mn = map(row_number(), ~ rnorm(100, mean = .x + rnorm(10, sd = 1)))
 ) |> 
  tidyr::unnest(mn) |> 
 summarize(
  estimate = median(mn) / 100,
  lower = quantile(mn, prob = 0.025) / 100,
  upper = quantile(mn, prob = 0.975) / 100,
  .by = c(model)
 ) |> 
 mutate(model = reorder(model, estimate)) |> 
 ggplot(aes(y = model)) + 
 geom_point(aes(x = estimate)) + 
 geom_errorbar(aes(xmin = lower, xmax = upper), width = 1/ 2) +
 scale_x_continuous(labels = label_percent()) +
 labs(y = NULL, x = "Difference in Accuracy from Previous Best LLM") +
 theme_bw()
```


## inspect and vitals 

[`inspect`](https://inspect.aisi.org.uk/) is "a framework for large language model evaluations created by the UK AI Security Institute."

<br> 

[`vitals`](vitals.tidyverse.org) is an R package that treats LLM evaluations similarly to unit testing. 

<br>

We'll use example data from `vitals` to illustrate some of the statistical issues. 

- `r vctrs::vec_unique_count(are_eval$id)` R-related tasks/questions
- Three LLMS: `r cli::format_inline("{levels(are_eval$LLM)}")`
- Each question is run three times for each LLM

## Terminology/notation

For our data, there are three _ordinal_ levels: incorrect (`I`), partially correct (`P`), and correct (`C`).

Some notation: 

- $C$ outcome values ($C = 3$ here) with `I` < `P` < `C`
- $p$ LLMs ($p=3$ for our data)
- $m$ epochs (a.k.a. replicates, $m = 3$)


## Data Types

This talk focuses on ordinal outcomes but the methods are easily extended to other outcome encodings: 

- Binary data  (Pass/fail):  logistic regression.
- Ranked by correctness: rank regression, Bradley-Terry Model
- Bounded scale (e.g. 0% t 100% correct): Beta regression

and so on. 

## Our Example Data

```{r}
#| label: tile-plot
#| echo: false
#| fig-width: 8
#| fig-height: 5
#| fig-align: "center"
#| out-width: "80%"
#| 
are_eval %>%
  ggplot(aes(x = epoch, y = id)) +
  geom_tile(aes(fill = score), alpha = 2 / 3) +
  facet_wrap(~ LLM, ncol = 3) +
  scale_fill_brewer(palette = "Set1") +
  labs(x = "Epoch", y = NULL) +
  theme(
    legend.position = "top",
    strip.text = element_text(size = 8)
  )
```


## Anthropic Manuscript

["Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/pdf/2411.00640) was released in 2024 and does a good job of describing _the spirit_ of how to analyze these results. 

<br>

It shows equations for computing large-sample standard errors for proportions (which are assumed to be normal). 

<br>

The manuscript is not wrong statistically, but very myopic and not especially good statistical methodology. 


# We need statistical inference  {background-color="#D04E59FF"}


## Unsexy, existing statistical models to the rescue

Generalized linear models have existed _for 51 years_ and solve nearly all the variations of this problem. 

<br> 

This experimental design falls squarely into the realm of analysis of variance (ANOVA). 

<br> 

"Modern" statistical tools for basic or more advanced models have evolved into frameworks that can seamlessly handle different outcome types of designs. 

## Correlated Data

The primary complication here is the question-to-question variability. 

<br> 

The rows of data are not statistically independent; some rows are more correlated than others. 

<br> 

The questions are a sample of larger questions, so we can treat this as a _random_ effect in the analysis to get proper inference. 

<br> 

We'll take a minimalist approach for this and use a _random intercept model_ to handle the statistical aspects 

## More Complex Experiments

We could also have random terms for raters, and other systematic effects that are samples from a population. 

We also analyze multiple levels, such as version with model (or families): 

- GPT
	- v3.5, v4, v4o
- Claude
	- v3-Opus, v3-Sonnet, v3-Haiku, v2.1, v2
- Gemini
	- v2.5-Pro, v2.5-Flash, c2.5-Flash-Lite

and so on

# The proportional odds model  {background-color="#95C65CFF"}

## The Proportional Odds Model

To model the score probabilities, the cumulative logit model estimates $C-1$ outcomes of

$$
\log\left(\frac{Pr[Y_{i} \ge c]}{1 - Pr[Y_{i} \ge c]}\right) = (\theta_c - \alpha_{k})- (\beta_2x_{i2} + \ldots +  \beta_{p}x_{ip})
$$

::: {.column width="50%"}
- $c = 1\ldots C$ outcomes (correct, ...)
- $i=1\ldots n$ results
- $j=2\ldots p$ LLMs
- $k=1\ldots m$ epochs/replicates
:::

::: {.column width="50%"}
- $\theta_c$: differences in outcome levels
- $\alpha_{k}$: within-question correlation
- $\beta_{j}$: contrasts LLMs
   -  $\beta_{2}$: GPT - Gemini
   -  $\beta_{3}$: GPT - Claude
:::

## Hierarchical (mixed) Model

Estimate parameters using maximum likelihood. Straightforward-ish. 

<br> 

Enables inference via p-values (booo) and confidence intervals (better).

<br>

```{r}
#| label: mle
#| eval: false
#| code-line-numbers: "2|4-5|7|"
# `id` is the question number
llm_mod  <- ordinal::clmm(score ~ LLM + (1|id), data = res, Hess = TRUE)

null_mod <- ordinal::clmm(score ~ 1   + (1|id), data = res, Hess = TRUE)
anova(llm_mod, null_mod)  # Bad news: p-value is 0.2726 for these data

broom::tidy(llm_mod, conf.int = TRUE)
```

```{r}
#| label: mle-comps
#| results: hide
#| echo: false

multiple_mod <- clmm(score ~ LLM + (1|id), data = are_eval, Hess = TRUE)
multiple_null <- clmm(score ~ 1 + (1|id), data = are_eval, Hess = TRUE)

multiple_lrt <- anova(multiple_null, multiple_mod)

multiple_coef <- 
 multiple_mod %>% 
 tidy(conf.int = TRUE, conf.level = 0.9) %>% 
 mutate(
  LLM = gsub("LLM", "", term),
  parameter = ifelse(coef.type == "intercept", "theta", "beta")
 ) 

mle_odds <- 
 multiple_coef %>% 
 filter(!grepl("\\|", LLM)) %>% 
 select(LLM, estimate, lower = conf.low, upper = conf.high) %>% 
 mutate(
  odds_estimate = exp(estimate),
  odds_lower = exp(lower),
  odds_upper = exp(upper)
 ) |> 
 select(-estimate, -lower, -upper)

multiple_coef <- full_join(multiple_coef, mle_odds, by = "LLM")

multiple_intercepts <- 
 are_eval %>% 
 distinct(id) %>%
 mutate(
  effect = multiple_mod$ranef,
  # reorder the ids by the magnitude of the effect associated with them
  id = factor(id),
  id = reorder(id, effect)
 )

difficult <- 
 multiple_intercepts %>% 
 slice_min(effect, n = 1, with_ties = FALSE) %>% 
 inner_join(are_eval, by = "id")

difficult_xtab <- count(difficult, score)
difficult_text <- 
 format_inline("{sum(difficult_xtab$n[difficult_xtab$score == 'C'])} of {difficult_xtab$n}")

effect_range <- max(extendrange(abs(multiple_intercepts$effect)))
```

## Results

Parameter estimates are the difference from the `r levels(are_eval$LLM)[1]` results:

```{r}
#| label: mle-beta
#| echo: false

multiple_coef %>%
 select(-std.error, -statistic, -term, -coef.type, -parameter) |> 
 filter(!grepl("\\|", LLM)) |> 
 relocate(LLM) |> 
 gt() %>%
 tab_spanner(
  label = "Parameters",
  columns = c(estimate, conf.low, conf.high, p.value)
 ) %>%
 tab_spanner(
  label = "Odds Ratios",
  columns = c(odds_estimate, odds_lower, odds_upper)
 ) %>%
 cols_label(
  contains("estimate") ~ "Estimate",
  contains("p.value") ~ "p-Value",
  contains("lower") ~ "Lower 95%",
  contains("upper") ~ "Lower 95%",
  contains("conf") ~ "Lower 95%"
 ) %>%
 fmt_number(columns = c(-LLM), n_sigfig = 3) |> 
 tab_options(table.font.size = px(30)) 
```

. . .

IMO inference is torturous: 

. . .

> If we were to repeat this experiment a large number of times, the true parameter value of the difference between `r levels(are_eval$LLM)[1]` and `r levels(are_eval$LLM)[2]` would fall between `r round(multiple_coef$conf.low[4], 3)` and `r round(multiple_coef$conf.high[4], 3)` 95% of the time.

## Difficulty Estimates a.k.a. BLUPs

```{r}
#| label: mle-tasks
#| echo: false
#| fig-width: 7
#| fig-height: 4
#| fig-align: "center"
#| out-width: "80%"

multiple_intercepts %>% 
 ggplot(aes(x = effect, y = id)) + 
 geom_point() +
 labs(y = NULL, x = "Random Intercept Estimate\n(incorrect <----------------> correct)") +
 lims(x = c(-effect_range, effect_range)) +
 theme_bw()
```


## Bayesian Hierarchical Model

Requires priors for parameters and estimation via MCMC.

<br> 

Inference is _rational_ via probability statements. 

<br>

Code is not as simple, but `brms` has a high-level interface
```{r}
#| label: bayes-comps-show
#| eval: false
#| line-numbers: false
brm(
	score ~ LLM + (1 | id),
	data = res,
	family = cumulative(link = "logit", threshold = "flexible"),
	prior = c(set_prior(prior = "student_t(1, 0, 1)", class = "Intercept")),
	# sampling options such as `chains`, `iter`, etc.
)
```


```{r}
#| label: bayes-comps
#| results: hide
#| echo: false
#| cache: true
multiple_bayes <-
 brm(
  score ~ LLM + (1|id),
  data = are_eval,
  family = cumulative(link = "logit", threshold = "flexible"),
  prior = c(set_prior(prior = "student_t(1, 0, 1)", class = "Intercept")),
  chains = 10,
  iter = 10000,
  cores = 10,
  seed = 410
 )

set.seed(280)

all_post <- as_draws_df(multiple_bayes)

bayes_regression_param <- 
 all_post %>%
 select(contains("b_LLM")) %>% 
 pivot_longer(
  cols = c(everything()),
  names_to = "param_name",
  values_to = "value"
 ) %>% 
 mutate(
  LLM = 
   case_when(
    param_name == "b_LLMGemini2.5Pro" ~ "Gemini 2.5 Pro",
    param_name == "b_LLMClaude4Sonnet" ~ "Claude 4 Sonnet"
   ),
  LLM = factor(LLM)
 ) 

bayes_reg_summary <- 
 bayes_regression_param %>% 
 summarize(
  mean = mean(value),
  lower = quantile(value, 0.05),
  upper = quantile(value, 0.95),
  mean_odds = mean(exp(value)),
  lower_odds = quantile(exp(value), 0.05),
  upper_odds = quantile(exp(value), 0.95),
  .by = c(LLM)
 )

post_claude <- bayes_regression_param$value[bayes_regression_param$LLM == "Claude 4 Sonnet"]
o2_le_zero <- mean(abs(post_claude) < 0.05) * 100

bayes_intercepts <- 
 all_post %>%
 select(contains("r_id")) %>%
 pivot_longer(
  cols = c(everything()),
  names_to = "sample",
  values_to = "value"
 ) %>%
 summarize(
  mean = mean(value),
  lower = quantile(value, prob = 0.05),
  upper = quantile(value, prob = 0.95),
  .by = c(sample)
 ) %>%
 mutate(
  id = gsub(",Intercept]", "", sample, fixed = TRUE),
  id = gsub("r_id[", "", id, fixed = TRUE),
  id = factor(id)
 ) %>%
 arrange(id) %>% 
 inner_join(
  are_eval %>% distinct(id), by = "id"
 ) %>% 
 mutate(
  id = as.character(id),
  id = reorder(id, mean)
 )

bayes_effect_range <- 
 bayes_intercepts %>% 
 select(lower, upper) %>% 
 unlist() %>% 
 abs() %>% 
 extendrange() %>% 
 max()

# For density plot
param_post <-
  all_post %>%
  select(contains("b_LLM")) %>%
  pivot_longer(
    cols = c(everything()),
    names_to = "param_name",
    values_to = "value"
  ) %>%
  filter(param_name == "b_LLMClaude4Sonnet") |>
  mutate(comparison = "Claude Better Than GPT", param_name = "")

quant <- mean(param_post$value < 0)
```


## Bayesian Results

<br> 

```{r}
#| label: bayes-beta
#| echo: false

bayes_reg_summary %>%
 gt() %>%
 tab_spanner(
  label = "Parameters",
  columns = c(mean, lower, upper)
 ) %>%
 tab_spanner(
  label = "Odds Ratios",
  columns = c(mean_odds, lower_odds, upper_odds)
 ) %>%
 cols_label(
  starts_with("mean") ~ "Mean",
  starts_with("lower") ~ "5% Percentile",
  starts_with("upper") ~ "95% Percentile"
 ) %>%
 fmt_number(columns = c(-LLM), n_sigfig = 3) |> 
 tab_options(table.font.size = px(26)) 
```

<br> 

. . .

Inference is incredibly simple: 

. . .


:::: {.columns}

::: {.column width="70%"}

> There is a `r round(mean(all_post$b_LLMClaude4Sonnet > 0)* 100, 1)`% probability that `r levels(are_eval$LLM)[3]` is better than `r levels(are_eval$LLM)[1]`.
:::

::: {.column width="30%"}

```{r}
#| label: desn
#| echo: false
#| out-width: 90%
#| fig-width: 5
#| fig-height: 3
#| message: false
#| warning: false

param_post |>
  ggplot(aes(value, param_name, fill = after_stat(quantile))) +
  stat_density_ridges(
    geom = "density_ridges_gradient",
    calc_ecdf = TRUE,
    quantiles = quant,
    show.legend = FALSE,
  ) +
  scale_fill_viridis_d(name = "Percentile", labels = c("<95", ">95")) +
  theme_minimal() +
  labs(x = "Claude Better Than GPT", y = NULL) +
  lims(x = c(-0.7, 1.9))
```
:::

::::

## Bayesian difficulty Estimates

```{r}
#| label: bayes-tasks
#| echo: false
#| fig-width: 7
#| fig-height: 4
#| fig-align: "center"
#| out-width: "80%"

bayes_intercepts %>%
 ggplot(aes(y = id)) +
 geom_point(aes(x = mean)) +
 geom_errorbar(aes(xmin = lower, xmax = upper), width = 1 / 2) +
 labs(y = NULL, x = "Random Intercept Estimate\n(incorrect <----------------> correct)") +
 lims(x = c(-bayes_effect_range, bayes_effect_range)) +
 theme_bw()
```


## Summary

It might be a good idea to consult a statistician for advice on conducting an inferential analysis of your experimental design. 

<br> 

Existing statistical tools perform very well for any type of evaluation outcome and can accommodate nearly every experimental design.

<br> 

R has very good existing tools for executing the analyses. 

# Thanks for listening! {background-color="#FAE093FF"}
